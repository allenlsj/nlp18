{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import pickle as pkl\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(folder_path, label): \n",
    "    scores = []\n",
    "    data_list = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            scores.append(int(file[file.find(\"_\")+1:file.find(\".\")]))\n",
    "            with open(folder_path+file) as f:\n",
    "                data_list.append(f.read())\n",
    "    \n",
    "    labels = label*np.ones(len(scores))\n",
    "    return data_list, labels, scores\n",
    "\n",
    "def merge_data(data1, data2, label1, label2, score1, score2, split, shuffle, train_size=20000):\n",
    "    data = data1+data2\n",
    "    labels = np.concatenate([label1, label2]).tolist()\n",
    "    scores = score1+score2\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.seed(0)\n",
    "        index = np.random.permutation(len(data))\n",
    "        data = np.array(data)[index].tolist()\n",
    "        labels = np.array(labels)[index].tolist()\n",
    "        scores = np.array(scores)[index].tolist()\n",
    "    \n",
    "    if split:\n",
    "        train_data = data[:train_size]\n",
    "        val_data = data[train_size:]\n",
    "        train_labels = labels[:train_size]\n",
    "        val_labels = labels[train_size:]\n",
    "        train_scores = scores[:train_size]\n",
    "        val_scores = scores[train_size:]\n",
    "        return train_data, train_labels, train_scores, val_data, val_labels, val_scores\n",
    "    \n",
    "    return data, labels, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_path = os.getcwd()+'/aclImdb/train/pos/'\n",
    "train_neg_path = os.getcwd()+'/aclImdb/train/neg/'\n",
    "test_pos_path = os.getcwd()+'/aclImdb/test/pos/'\n",
    "test_neg_path = os.getcwd()+'/aclImdb/test/neg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_data, train_pos_label, train_pos_scores = load_data(train_pos_path, 1)\n",
    "train_neg_data, train_neg_label, train_neg_scores = load_data(train_neg_path, 0)\n",
    "test_pos_data, test_pos_label, test_pos_scores = load_data(test_pos_path, 1)\n",
    "test_neg_data, test_neg_label, test_neg_scores = load_data(test_neg_path, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, train_labels, train_scores, \\\n",
    "val_data, val_labels, val_scores = merge_data(train_pos_data, train_neg_data, train_pos_label, train_neg_label,\n",
    "                                              train_pos_scores, train_neg_scores, True, True)\n",
    "test_data, test_labels, test_scores = \\\n",
    "merge_data(test_pos_data, test_neg_data, test_pos_label, test_neg_label,\n",
    "                                              test_pos_scores, test_neg_scores, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def tokenize(sent, tokenization):\n",
    "    tokens = tokenizer(sent)\n",
    "    if tokenization:\n",
    "        return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    else:\n",
    "        return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenization):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample, tokenization)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4809135\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab lists and transform data into indices lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n1, id2token_n1 = build_vocab(all_train_tokens, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n1 = token2index_dataset(train_data_tokens, token2id_n1)\n",
    "val_data_indices_n1 = token2index_dataset(val_data_tokens, token2id_n1)\n",
    "test_data_indices_n1 = token2index_dataset(test_data_tokens, token2id_n1)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n1)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n1)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when yo-u call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores = [i-1 for i in train_scores]\n",
    "val_scores = [i-1 for i in val_scores]\n",
    "test_scores = [i-1 for i in test_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n1 = NewsGroupDataset(train_data_indices_n1, train_scores)\n",
    "train_loader_n1 = torch.utils.data.DataLoader(dataset=train_dataset_n1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n1 = NewsGroupDataset(val_data_indices_n1, val_scores)\n",
    "val_loader_n1 = torch.utils.data.DataLoader(dataset=val_dataset_n1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n1 = NewsGroupDataset(test_data_indices_n1, test_scores)\n",
    "test_loader_n1 = torch.utils.data.DataLoader(dataset=test_dataset_n1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of N-gram regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BagOfNgram(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfNgram classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim, train_scores):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNgram, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.predict = nn.Linear(emb_dim, 10)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.predict(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, scores in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, scores\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += scores.size(0)\n",
    "        correct += predicted.eq(scores.view_as(predicted)).sum().item()\n",
    "    \n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_proc(model, train_loader, val_loader, lr, adj, ep, optim, lr_decay=0, plt=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if optim == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    if adj:\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=lr_decay)\n",
    "    \n",
    "    train_ls = []\n",
    "    for epoch in range(ep):\n",
    "        if adj:\n",
    "            scheduler.step()\n",
    "        for i, (data, lengths, scores) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, score_batch = data, lengths, scores\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "#             print(outputs)\n",
    "#             print(score_batch)\n",
    "            loss = criterion(outputs, score_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_ls.append(loss)\n",
    "            \n",
    "#             if i > 0 and i % 100 == 0:\n",
    "#                 val_acc, val_loss = test_model(val_loader, model)\n",
    "#                 val_ls += val_loss\n",
    "#                 print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "#                     epoch+1, ep, i+1, len(train_loader), val_acc))\n",
    "    \n",
    "    val_acc  = test_model(val_loader, model)\n",
    "    print('Val Accuracy: {}'.format(val_acc))\n",
    "    \n",
    "    if plt:\n",
    "        plt.plot(train_ls)\n",
    "        plt.xlabel(\"n\")\n",
    "        plt.ylabel(\"Train Loss\")\n",
    "    \n",
    "    return val_acc, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 42.14\n"
     ]
    }
   ],
   "source": [
    "emb_size = 100\n",
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size, train_scores)\n",
    "val_acc, model = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:41.276\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "index = []\n",
    "for data, lengths, scores in test_loader_n1:\n",
    "    data_batch, length_batch, label_batch = data, lengths, scores\n",
    "    outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "    predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "    total += scores.size(0)\n",
    "    correct += predicted.eq(scores.view_as(predicted)).sum().item()\n",
    "    \n",
    "print(\"Test Accuracy:{}\".format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
