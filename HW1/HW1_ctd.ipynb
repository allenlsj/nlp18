{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import pickle as pkl\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(folder_path, label): \n",
    "    scores = []\n",
    "    data_list = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            scores.append(int(file[file.find(\"_\")+1:file.find(\".\")]))\n",
    "            with open(folder_path+file) as f:\n",
    "                data_list.append(f.read())\n",
    "    \n",
    "    labels = label*np.ones(len(scores))\n",
    "    return data_list, labels, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_data(data1, data2, label1, label2, score1, score2, split, shuffle, train_size=20000):\n",
    "    data = data1+data2\n",
    "    labels = np.concatenate([label1, label2]).tolist()\n",
    "    scores = score1+score2\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.seed(0)\n",
    "        index = np.random.permutation(len(data))\n",
    "        data = np.array(data)[index].tolist()\n",
    "        labels = np.array(labels)[index].tolist()\n",
    "        scores = np.array(scores)[index].tolist()\n",
    "    \n",
    "    if split:\n",
    "        train_data = data[:train_size]\n",
    "        val_data = data[train_size:]\n",
    "        train_labels = labels[:train_size]\n",
    "        val_labels = labels[train_size:]\n",
    "        train_scores = scores[:train_size]\n",
    "        val_scores = scores[train_size]\n",
    "        return train_data, train_labels, train_scores, val_data, val_labels, val_scores\n",
    "    \n",
    "    return data, labels, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_path = os.getcwd()+'/aclImdb/train/pos/'\n",
    "train_neg_path = os.getcwd()+'/aclImdb/train/neg/'\n",
    "test_pos_path = os.getcwd()+'/aclImdb/test/pos/'\n",
    "test_neg_path = os.getcwd()+'/aclImdb/test/neg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_data, train_pos_label, train_pos_scores = load_data(train_pos_path, 1)\n",
    "train_neg_data, train_neg_label, train_neg_scores = load_data(train_neg_path, 0)\n",
    "test_pos_data, test_pos_label, test_pos_scores = load_data(test_pos_path, 1)\n",
    "test_neg_data, test_neg_label, test_neg_scores = load_data(test_neg_path, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_data, train_labels, train_scores, \\\n",
    "# val_data, val_labels, val_scores = merge_data(train_pos_data, train_neg_data, train_pos_label, train_neg_label,\n",
    "#                                               train_pos_scores, train_neg_scores, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data, test_labels, test_scores = \\\n",
    "merge_data(test_pos_data, test_neg_data, test_pos_label, test_neg_label,\n",
    "                                              test_pos_scores, test_neg_scores, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pkl.dump(train_data, open(\"train_data.p\", \"wb\"))\n",
    "# pkl.dump(train_labels, open(\"train_labels.p\", \"wb\"))\n",
    "# pkl.dump(train_scores, open(\"train_scores.p\", \"wb\"))\n",
    "# pkl.dump(val_data, open(\"val_data.p\", \"wb\"))\n",
    "# pkl.dump(val_labels, open(\"val_labels.p\", \"wb\"))\n",
    "# pkl.dump(val_scores, open(\"val_scores.p\", \"wb\"))\n",
    "train_data = pkl.load(open(\"train_data.p\", \"rb\"))\n",
    "train_labels = pkl.load(open(\"train_labels.p\", \"rb\"))\n",
    "train_scores = pkl.load(open(\"train_scores.p\", \"rb\"))\n",
    "val_data = pkl.load(open(\"val_data.p\", \"rb\"))\n",
    "val_labels = pkl.load(open(\"val_labels.p\", \"rb\"))\n",
    "val_scores = pkl.load(open(\"val_scores.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def tokenize(sent, tokenization):\n",
    "    tokens = tokenizer(sent)\n",
    "    if tokenization:\n",
    "        return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    else:\n",
    "        return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenization):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample, tokenization)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4809135\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word) no tokenization (lowercase & remove punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5439707\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_ntk = pkl.load(open(\"train_data_tokens_ntk.p\", \"rb\"))\n",
    "all_train_tokens_ntk = pkl.load(open(\"all_train_tokens_ntk.p\", \"rb\"))\n",
    "val_data_tokens_ntk = pkl.load(open(\"val_data_tokens_ntk.p\", \"rb\"))\n",
    "test_data_tokens_ntk = pkl.load(open(\"test_data_tokens_ntk.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_ntk)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset_ngram(dataset, n, tokenization):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample, tokenization)\n",
    "        n_grams = list(ngrams(tokens, n))\n",
    "        token_dataset.append(n_grams)\n",
    "        all_tokens += n_grams\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4789135\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n2 = pkl.load(open(\"train_data_tokens_n2.p\", \"rb\"))\n",
    "all_train_tokens_n2 = pkl.load(open(\"all_train_tokens_n2.p\", \"rb\"))\n",
    "val_data_tokens_n2 = pkl.load(open(\"val_data_tokens_n2.p\", \"rb\"))\n",
    "test_data_tokens_n2 = pkl.load(open(\"test_data_tokens_n2.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n2)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n2)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n2)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams no tokenization (lowercase & remove punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5419707\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n2_ntk = pkl.load(open(\"train_data_tokens_n2_ntk.p\", \"rb\"))\n",
    "all_train_tokens_n2_ntk = pkl.load(open(\"all_train_tokens_n2_ntk.p\", \"rb\"))\n",
    "val_data_tokens_n2_ntk = pkl.load(open(\"val_data_tokens_n2_ntk.p\", \"rb\"))\n",
    "test_data_tokens_n2_ntk = pkl.load(open(\"test_data_tokens_n2_ntk.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n2_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n2_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n2_ntk)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n2_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4769135\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n3 = pkl.load(open(\"train_data_tokens_n3.p\", \"rb\"))\n",
    "all_train_tokens_n3 = pkl.load(open(\"all_train_tokens_n3.p\", \"rb\"))\n",
    "val_data_tokens_n3 = pkl.load(open(\"val_data_tokens_n3.p\", \"rb\"))\n",
    "test_data_tokens_n3 = pkl.load(open(\"test_data_tokens_n3.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n3)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n3)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n3)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams no tokenization (lowercase & remove punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5399707\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n3_ntk = pkl.load(open(\"train_data_tokens_n3_ntk.p\", \"rb\"))\n",
    "all_train_tokens_n3_ntk = pkl.load(open(\"all_train_tokens_n3_ntk.p\", \"rb\"))\n",
    "val_data_tokens_n3_ntk = pkl.load(open(\"val_data_tokens_n3_ntk.p\", \"rb\"))\n",
    "test_data_tokens_n3_ntk = pkl.load(open(\"test_data_tokens_n3_ntk.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n3_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n3_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n3_ntk)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n3_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4749135\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n4 = pkl.load(open(\"train_data_tokens_n4.p\", \"rb\"))\n",
    "all_train_tokens_n4 = pkl.load(open(\"all_train_tokens_n4.p\", \"rb\"))\n",
    "val_data_tokens_n4 = pkl.load(open(\"val_data_tokens_n4.p\", \"rb\"))\n",
    "test_data_tokens_n4 = pkl.load(open(\"test_data_tokens_n4.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n4)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n4)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n4)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams no tokenization (lowercase & remove punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5379707\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n4_ntk = pkl.load(open(\"train_data_tokens_n4_ntk.p\", \"rb\"))\n",
    "all_train_tokens_n4_ntk = pkl.load(open(\"all_train_tokens_n4_ntk.p\", \"rb\"))\n",
    "val_data_tokens_n4_ntk = pkl.load(open(\"val_data_tokens_n4_ntk.p\", \"rb\"))\n",
    "test_data_tokens_n4_ntk = pkl.load(open(\"test_data_tokens_n4_ntk.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n4_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n4_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n4_ntk)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n4_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab lists and transform data into indices lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n1, id2token_n1 = build_vocab(all_train_tokens, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n1 = token2index_dataset(train_data_tokens, token2id_n1)\n",
    "val_data_indices_n1 = token2index_dataset(val_data_tokens, token2id_n1)\n",
    "test_data_indices_n1 = token2index_dataset(test_data_tokens, token2id_n1)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n1)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n1)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word) no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n1_ntk, id2token_n1_ntk = build_vocab(all_train_tokens_ntk, max_vocab_size = 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n1_ntk = token2index_dataset(train_data_tokens_ntk, token2id_n1_ntk)\n",
    "val_data_indices_n1_ntk = token2index_dataset(val_data_tokens_ntk, token2id_n1_ntk)\n",
    "test_data_indices_n1_ntk = token2index_dataset(test_data_tokens_ntk, token2id_n1_ntk)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n1_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n1_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n1_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n2, id2token_n2 = build_vocab(all_train_tokens_n2, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n2 = token2index_dataset(train_data_tokens_n2, token2id_n2)\n",
    "val_data_indices_n2 = token2index_dataset(val_data_tokens_n2, token2id_n2)\n",
    "test_data_indices_n2 = token2index_dataset(test_data_tokens_n2, token2id_n2)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n2)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n2)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n2_ntk, id2token_n2_ntk = build_vocab(all_train_tokens_n2_ntk, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n2_ntk = token2index_dataset(train_data_tokens_n2_ntk, token2id_n2_ntk)\n",
    "val_data_indices_n2_ntk = token2index_dataset(val_data_tokens_n2_ntk, token2id_n2_ntk)\n",
    "test_data_indices_n2_ntk = token2index_dataset(test_data_tokens_n2_ntk, token2id_n2_ntk)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n2_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n2_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n2_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n3, id2token_n3 = build_vocab(all_train_tokens_n3, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n3 = token2index_dataset(train_data_tokens_n3, token2id_n3)\n",
    "val_data_indices_n3 = token2index_dataset(val_data_tokens_n3, token2id_n3)\n",
    "test_data_indices_n3 = token2index_dataset(test_data_tokens_n3, token2id_n3)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n3)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n3)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n3_ntk, id2token_n3_ntk = build_vocab(all_train_tokens_n3_ntk, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n3_ntk = token2index_dataset(train_data_tokens_n3_ntk, token2id_n3_ntk)\n",
    "val_data_indices_n3_ntk = token2index_dataset(val_data_tokens_n3_ntk, token2id_n3_ntk)\n",
    "test_data_indices_n3_ntk = token2index_dataset(test_data_tokens_n3_ntk, token2id_n3_ntk)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n3_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n3_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n3_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n4, id2token_n4 = build_vocab(all_train_tokens_n4, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n4 = token2index_dataset(train_data_tokens_n4, token2id_n4)\n",
    "val_data_indices_n4 = token2index_dataset(val_data_tokens_n4, token2id_n4)\n",
    "test_data_indices_n4 = token2index_dataset(test_data_tokens_n4, token2id_n4)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n4)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n4)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n4_ntk, id2token_n4_ntk = build_vocab(all_train_tokens_n4_ntk, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n4_ntk = token2index_dataset(train_data_tokens_n4_ntk, token2id_n4_ntk)\n",
    "val_data_indices_n4_ntk = token2index_dataset(val_data_tokens_n4_ntk, token2id_n4_ntk)\n",
    "test_data_indices_n4_ntk = token2index_dataset(test_data_tokens_n4_ntk, token2id_n4_ntk)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n4_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n4_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n4_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when yo-u call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1-gram (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n1 = NewsGroupDataset(train_data_indices_n1, train_labels)\n",
    "train_loader_n1 = torch.utils.data.DataLoader(dataset=train_dataset_n1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n1 = NewsGroupDataset(val_data_indices_n1, val_labels)\n",
    "val_loader_n1 = torch.utils.data.DataLoader(dataset=val_dataset_n1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n1 = NewsGroupDataset(test_data_indices_n1, test_labels)\n",
    "test_loader_n1 = torch.utils.data.DataLoader(dataset=test_dataset_n1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word) no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n1_ntk = NewsGroupDataset(train_data_indices_n1_ntk, train_labels)\n",
    "train_loader_n1_ntk = torch.utils.data.DataLoader(dataset=train_dataset_n1_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n1_ntk = NewsGroupDataset(val_data_indices_n1_ntk, val_labels)\n",
    "val_loader_n1_ntk = torch.utils.data.DataLoader(dataset=val_dataset_n1_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n1_ntk = NewsGroupDataset(test_data_indices_n1_ntk, test_labels)\n",
    "test_loader_n1_ntk = torch.utils.data.DataLoader(dataset=test_dataset_n1_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n2 = NewsGroupDataset(train_data_indices_n2, train_labels)\n",
    "train_loader_n2 = torch.utils.data.DataLoader(dataset=train_dataset_n2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n2 = NewsGroupDataset(val_data_indices_n2, val_labels)\n",
    "val_loader_n2 = torch.utils.data.DataLoader(dataset=val_dataset_n2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n2 = NewsGroupDataset(test_data_indices_n2, test_labels)\n",
    "test_loader_n2 = torch.utils.data.DataLoader(dataset=test_dataset_n2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n2_ntk = NewsGroupDataset(train_data_indices_n2_ntk, train_labels)\n",
    "train_loader_n2_ntk = torch.utils.data.DataLoader(dataset=train_dataset_n2_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n2_ntk = NewsGroupDataset(val_data_indices_n2_ntk, val_labels)\n",
    "val_loader_n2_ntk = torch.utils.data.DataLoader(dataset=val_dataset_n2_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n2_ntk = NewsGroupDataset(test_data_indices_n2_ntk, test_labels)\n",
    "test_loader_n2_ntk = torch.utils.data.DataLoader(dataset=test_dataset_n2_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n3 = NewsGroupDataset(train_data_indices_n3, train_labels)\n",
    "train_loader_n3 = torch.utils.data.DataLoader(dataset=train_dataset_n3, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n3 = NewsGroupDataset(val_data_indices_n3, val_labels)\n",
    "val_loader_n3 = torch.utils.data.DataLoader(dataset=val_dataset_n3, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n3 = NewsGroupDataset(test_data_indices_n3, test_labels)\n",
    "test_loader_n3 = torch.utils.data.DataLoader(dataset=test_dataset_n3, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n3_ntk = NewsGroupDataset(train_data_indices_n3_ntk, train_labels)\n",
    "train_loader_n3_ntk = torch.utils.data.DataLoader(dataset=train_dataset_n3_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n3_ntk = NewsGroupDataset(val_data_indices_n3_ntk, val_labels)\n",
    "val_loader_n3_ntk = torch.utils.data.DataLoader(dataset=val_dataset_n3_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n3_ntk = NewsGroupDataset(test_data_indices_n3_ntk, test_labels)\n",
    "test_loader_n3_ntk = torch.utils.data.DataLoader(dataset=test_dataset_n3_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n4 = NewsGroupDataset(train_data_indices_n4, train_labels)\n",
    "train_loader_n4 = torch.utils.data.DataLoader(dataset=train_dataset_n4, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n4 = NewsGroupDataset(val_data_indices_n4, val_labels)\n",
    "val_loader_n4 = torch.utils.data.DataLoader(dataset=val_dataset_n4, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n4 = NewsGroupDataset(test_data_indices_n4, test_labels)\n",
    "test_loader_n4 = torch.utils.data.DataLoader(dataset=test_dataset_n4, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n4_ntk = NewsGroupDataset(train_data_indices_n4_ntk, train_labels)\n",
    "train_loader_n4_ntk = torch.utils.data.DataLoader(dataset=train_dataset_n4_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n4_ntk = NewsGroupDataset(val_data_indices_n4_ntk, val_labels)\n",
    "val_loader_n4_ntk = torch.utils.data.DataLoader(dataset=val_dataset_n4_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n4_ntk = NewsGroupDataset(test_data_indices_n4_ntk, test_labels)\n",
    "test_loader_n4_ntk = torch.utils.data.DataLoader(dataset=test_dataset_n4_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of N-gram Model & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BagOfNgram(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfNgram classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNgram, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim, 2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        val_loss.append(criterion(model(data_batch, length_batch), label_batch).numpy())\n",
    "    \n",
    "    return (100 * correct / total), val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_proc(model, train_loader, val_loader, lr, adj, ep, optim, lr_decay=0, plt=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if optim == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    if adj:\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=lr_decay)\n",
    "    \n",
    "    val_ls = []\n",
    "    for epoch in range(ep):\n",
    "        if adj:\n",
    "            scheduler.step()\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "#             if i > 0 and i % 100 == 0:\n",
    "#                 val_acc, val_loss = test_model(val_loader, model)\n",
    "#                 val_ls += val_loss\n",
    "#                 print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "#                     epoch+1, ep, i+1, len(train_loader), val_acc))\n",
    "    \n",
    "    val_acc, _ = test_model(val_loader, model)\n",
    "    print('Val Accuracy: {}'.format(val_acc))\n",
    "    \n",
    "    if plt:\n",
    "        plt.plot(val_ls)\n",
    "        plt.xlabel(\"Epochs*2\")\n",
    "        plt.ylabel(\"Val Loss\")\n",
    "    \n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.32\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.72\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.14\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 88.06\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.32\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 88.4\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 65.84\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 63.42\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.9\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 62.34\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.1\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 63.18\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.54\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.7\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.88\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 88.42\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.36\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 88.3\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 69.42\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 65.52\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.22\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 64.94\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 67.18\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 67.72\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word) no tokenizaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.12\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.94\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.1\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 88.06\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.66\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.78\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.94\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 61.2\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 67.02\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 60.72\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 65.66\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 62.52\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.7\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.06\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.84\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.12\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.78\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.88\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 69.16\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 63.34\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.2\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 65.3\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 67.4\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.32\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.54\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.6\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.46\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.84\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.08\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.3\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 55.98\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 55.0\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 60.6\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.74\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 56.6\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 57.28\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 82.98\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.04\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 82.6\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.68\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.72\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.38\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 54.34\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.54\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.6\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 59.56\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 57.06\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 60.64\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.04\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.06\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.22\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.1\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.84\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.24\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 51.96\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.1\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 58.16\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.18\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 56.0\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 55.6\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 5, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.2\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.2\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 82.46\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.14\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.34\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.78\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 59.2\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 56.56\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 62.2\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 56.86\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 61.32\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 59.7\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 5, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.18\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.18\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.54\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.44\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 79.74\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 80.44\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.34\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.18\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 54.8\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.56\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.2\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 54.56\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.58\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.02\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.16\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 79.84\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 79.86\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 80.18\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 51.5\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.34\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.32\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.32\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.94\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.6\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.28\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.88\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.62\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 79.22\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 79.92\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.24\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.38\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.2\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.26\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 51.66\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.08\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.9\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.64\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.22\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 79.78\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.94\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 80.14\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 55.02\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.12\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 55.5\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.76\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.82\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.68\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.96\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.46\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.22\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 74.28\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 74.64\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.84\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.36\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.72\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.56\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.06\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.48\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.9\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 74.46\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 74.58\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.84\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.18\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.84\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.36\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 73.48\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.1\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.3\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.52\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 73.34\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 73.28\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.72\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.74\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.26\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.84\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.54\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 51.74\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.18\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.52\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.0\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.4\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.84\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 73.62\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.88\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.18\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.14\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.6\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary (result of the best model on test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
