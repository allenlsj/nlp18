{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import pickle as pkl\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(folder_path, label): \n",
    "    scores = []\n",
    "    data_list = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            scores.append(int(file[file.find(\"_\")+1:file.find(\".\")]))\n",
    "            with open(folder_path+file) as f:\n",
    "                data_list.append(f.read())\n",
    "    \n",
    "    labels = label*np.ones(len(scores))\n",
    "    return data_list, labels, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_data(data1, data2, label1, label2, score1, score2, split, shuffle, train_size=20000):\n",
    "    data = data1+data2\n",
    "    labels = np.concatenate([label1, label2]).tolist()\n",
    "    scores = score1+score2\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.seed(0)\n",
    "        index = np.random.permutation(len(data))\n",
    "        data = np.array(data)[index].tolist()\n",
    "        labels = np.array(labels)[index].tolist()\n",
    "        scores = np.array(scores)[index].tolist()\n",
    "    \n",
    "    if split:\n",
    "        train_data = data[:train_size]\n",
    "        val_data = data[train_size:]\n",
    "        train_labels = labels[:train_size]\n",
    "        val_labels = labels[train_size:]\n",
    "        train_scores = scores[:train_size]\n",
    "        val_scores = scores[train_size]\n",
    "        return train_data, train_labels, train_scores, val_data, val_labels, val_scores\n",
    "    \n",
    "    return data, labels, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_path = os.getcwd()+'/aclImdb/train/pos/'\n",
    "train_neg_path = os.getcwd()+'/aclImdb/train/neg/'\n",
    "test_pos_path = os.getcwd()+'/aclImdb/test/pos/'\n",
    "test_neg_path = os.getcwd()+'/aclImdb/test/neg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_data, train_pos_label, train_pos_scores = load_data(train_pos_path, 1)\n",
    "train_neg_data, train_neg_label, train_neg_scores = load_data(train_neg_path, 0)\n",
    "test_pos_data, test_pos_label, test_pos_scores = load_data(test_pos_path, 1)\n",
    "test_neg_data, test_neg_label, test_neg_scores = load_data(test_neg_path, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_data, train_labels, train_scores, \\\n",
    "# val_data, val_labels, val_scores = merge_data(train_pos_data, train_neg_data, train_pos_label, train_neg_label,\n",
    "#                                               train_pos_scores, train_neg_scores, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data, test_labels, test_scores = \\\n",
    "merge_data(test_pos_data, test_neg_data, test_pos_label, test_neg_label,\n",
    "                                              test_pos_scores, test_neg_scores, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pkl.dump(train_data, open(\"train_data.p\", \"wb\"))\n",
    "# pkl.dump(train_labels, open(\"train_labels.p\", \"wb\"))\n",
    "# pkl.dump(train_scores, open(\"train_scores.p\", \"wb\"))\n",
    "# pkl.dump(val_data, open(\"val_data.p\", \"wb\"))\n",
    "# pkl.dump(val_labels, open(\"val_labels.p\", \"wb\"))\n",
    "# pkl.dump(val_scores, open(\"val_scores.p\", \"wb\"))\n",
    "train_data = pkl.load(open(\"train_data.p\", \"rb\"))\n",
    "train_labels = pkl.load(open(\"train_labels.p\", \"rb\"))\n",
    "train_scores = pkl.load(open(\"train_scores.p\", \"rb\"))\n",
    "val_data = pkl.load(open(\"val_data.p\", \"rb\"))\n",
    "val_labels = pkl.load(open(\"val_labels.p\", \"rb\"))\n",
    "val_scores = pkl.load(open(\"val_scores.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def tokenize(sent, tokenization):\n",
    "    tokens = tokenizer(sent)\n",
    "    if tokenization:\n",
    "        return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    else:\n",
    "        return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenization):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample, tokenization)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4809135\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word) no tokenization (lowercase & remove punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5439707\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_ntk = pkl.load(open(\"train_data_tokens_ntk.p\", \"rb\"))\n",
    "all_train_tokens_ntk = pkl.load(open(\"all_train_tokens_ntk.p\", \"rb\"))\n",
    "val_data_tokens_ntk = pkl.load(open(\"val_data_tokens_ntk.p\", \"rb\"))\n",
    "test_data_tokens_ntk = pkl.load(open(\"test_data_tokens_ntk.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_ntk)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset_ngram(dataset, n, tokenization):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample, tokenization)\n",
    "        n_grams = list(ngrams(tokens, n))\n",
    "        token_dataset.append(n_grams)\n",
    "        all_tokens += n_grams\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4789135\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n2 = pkl.load(open(\"train_data_tokens_n2.p\", \"rb\"))\n",
    "all_train_tokens_n2 = pkl.load(open(\"all_train_tokens_n2.p\", \"rb\"))\n",
    "val_data_tokens_n2 = pkl.load(open(\"val_data_tokens_n2.p\", \"rb\"))\n",
    "test_data_tokens_n2 = pkl.load(open(\"test_data_tokens_n2.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n2)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n2)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n2)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams no tokenization (lowercase & remove punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5419707\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n2_ntk = pkl.load(open(\"train_data_tokens_n2_ntk.p\", \"rb\"))\n",
    "all_train_tokens_n2_ntk = pkl.load(open(\"all_train_tokens_n2_ntk.p\", \"rb\"))\n",
    "val_data_tokens_n2_ntk = pkl.load(open(\"val_data_tokens_n2_ntk.p\", \"rb\"))\n",
    "test_data_tokens_n2_ntk = pkl.load(open(\"test_data_tokens_n2_ntk.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n2_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n2_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n2_ntk)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n2_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4769135\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n3 = pkl.load(open(\"train_data_tokens_n3.p\", \"rb\"))\n",
    "all_train_tokens_n3 = pkl.load(open(\"all_train_tokens_n3.p\", \"rb\"))\n",
    "val_data_tokens_n3 = pkl.load(open(\"val_data_tokens_n3.p\", \"rb\"))\n",
    "test_data_tokens_n3 = pkl.load(open(\"test_data_tokens_n3.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n3)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n3)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n3)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams no tokenization (lowercase & remove punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5399707\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n3_ntk = pkl.load(open(\"train_data_tokens_n3_ntk.p\", \"rb\"))\n",
    "all_train_tokens_n3_ntk = pkl.load(open(\"all_train_tokens_n3_ntk.p\", \"rb\"))\n",
    "val_data_tokens_n3_ntk = pkl.load(open(\"val_data_tokens_n3_ntk.p\", \"rb\"))\n",
    "test_data_tokens_n3_ntk = pkl.load(open(\"test_data_tokens_n3_ntk.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n3_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n3_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n3_ntk)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n3_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4749135\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n4 = pkl.load(open(\"train_data_tokens_n4.p\", \"rb\"))\n",
    "all_train_tokens_n4 = pkl.load(open(\"all_train_tokens_n4.p\", \"rb\"))\n",
    "val_data_tokens_n4 = pkl.load(open(\"val_data_tokens_n4.p\", \"rb\"))\n",
    "test_data_tokens_n4 = pkl.load(open(\"test_data_tokens_n4.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n4)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n4)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n4)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams no tokenization (lowercase & remove punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5379707\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n4_ntk = pkl.load(open(\"train_data_tokens_n4_ntk.p\", \"rb\"))\n",
    "all_train_tokens_n4_ntk = pkl.load(open(\"all_train_tokens_n4_ntk.p\", \"rb\"))\n",
    "val_data_tokens_n4_ntk = pkl.load(open(\"val_data_tokens_n4_ntk.p\", \"rb\"))\n",
    "test_data_tokens_n4_ntk = pkl.load(open(\"test_data_tokens_n4_ntk.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n4_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n4_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n4_ntk)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n4_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab lists and transform data into indices lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n1, id2token_n1 = build_vocab(all_train_tokens, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n1 = token2index_dataset(train_data_tokens, token2id_n1)\n",
    "val_data_indices_n1 = token2index_dataset(val_data_tokens, token2id_n1)\n",
    "test_data_indices_n1 = token2index_dataset(test_data_tokens, token2id_n1)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n1)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n1)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word) no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n1_ntk, id2token_n1_ntk = build_vocab(all_train_tokens_ntk, max_vocab_size = 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n1_ntk = token2index_dataset(train_data_tokens_ntk, token2id_n1_ntk)\n",
    "val_data_indices_n1_ntk = token2index_dataset(val_data_tokens_ntk, token2id_n1_ntk)\n",
    "test_data_indices_n1_ntk = token2index_dataset(test_data_tokens_ntk, token2id_n1_ntk)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n1_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n1_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n1_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n2, id2token_n2 = build_vocab(all_train_tokens_n2, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n2 = token2index_dataset(train_data_tokens_n2, token2id_n2)\n",
    "val_data_indices_n2 = token2index_dataset(val_data_tokens_n2, token2id_n2)\n",
    "test_data_indices_n2 = token2index_dataset(test_data_tokens_n2, token2id_n2)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n2)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n2)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n2_ntk, id2token_n2_ntk = build_vocab(all_train_tokens_n2_ntk, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n2_ntk = token2index_dataset(train_data_tokens_n2_ntk, token2id_n2_ntk)\n",
    "val_data_indices_n2_ntk = token2index_dataset(val_data_tokens_n2_ntk, token2id_n2_ntk)\n",
    "test_data_indices_n2_ntk = token2index_dataset(test_data_tokens_n2_ntk, token2id_n2_ntk)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n2_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n2_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n2_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n3, id2token_n3 = build_vocab(all_train_tokens_n3, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n3 = token2index_dataset(train_data_tokens_n3, token2id_n3)\n",
    "val_data_indices_n3 = token2index_dataset(val_data_tokens_n3, token2id_n3)\n",
    "test_data_indices_n3 = token2index_dataset(test_data_tokens_n3, token2id_n3)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n3)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n3)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n3_ntk, id2token_n3_ntk = build_vocab(all_train_tokens_n3_ntk, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n3_ntk = token2index_dataset(train_data_tokens_n3_ntk, token2id_n3_ntk)\n",
    "val_data_indices_n3_ntk = token2index_dataset(val_data_tokens_n3_ntk, token2id_n3_ntk)\n",
    "test_data_indices_n3_ntk = token2index_dataset(test_data_tokens_n3_ntk, token2id_n3_ntk)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n3_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n3_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n3_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n4, id2token_n4 = build_vocab(all_train_tokens_n4, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n4 = token2index_dataset(train_data_tokens_n4, token2id_n4)\n",
    "val_data_indices_n4 = token2index_dataset(val_data_tokens_n4, token2id_n4)\n",
    "test_data_indices_n4 = token2index_dataset(test_data_tokens_n4, token2id_n4)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n4)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n4)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 25000\n",
    "token2id_n4_ntk, id2token_n4_ntk = build_vocab(all_train_tokens_n4_ntk, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n4_ntk = token2index_dataset(train_data_tokens_n4_ntk, token2id_n4_ntk)\n",
    "val_data_indices_n4_ntk = token2index_dataset(val_data_tokens_n4_ntk, token2id_n4_ntk)\n",
    "test_data_indices_n4_ntk = token2index_dataset(test_data_tokens_n4_ntk, token2id_n4_ntk)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n4_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n4_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n4_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when yo-u call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1-gram (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n1 = NewsGroupDataset(train_data_indices_n1, train_labels)\n",
    "train_loader_n1 = torch.utils.data.DataLoader(dataset=train_dataset_n1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n1 = NewsGroupDataset(val_data_indices_n1, val_labels)\n",
    "val_loader_n1 = torch.utils.data.DataLoader(dataset=val_dataset_n1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n1 = NewsGroupDataset(test_data_indices_n1, test_labels)\n",
    "test_loader_n1 = torch.utils.data.DataLoader(dataset=test_dataset_n1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word) no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n1_ntk = NewsGroupDataset(train_data_indices_n1_ntk, train_labels)\n",
    "train_loader_n1_ntk = torch.utils.data.DataLoader(dataset=train_dataset_n1_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n1_ntk = NewsGroupDataset(val_data_indices_n1_ntk, val_labels)\n",
    "val_loader_n1_ntk = torch.utils.data.DataLoader(dataset=val_dataset_n1_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n1_ntk = NewsGroupDataset(test_data_indices_n1_ntk, test_labels)\n",
    "test_loader_n1_ntk = torch.utils.data.DataLoader(dataset=test_dataset_n1_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n2 = NewsGroupDataset(train_data_indices_n2, train_labels)\n",
    "train_loader_n2 = torch.utils.data.DataLoader(dataset=train_dataset_n2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n2 = NewsGroupDataset(val_data_indices_n2, val_labels)\n",
    "val_loader_n2 = torch.utils.data.DataLoader(dataset=val_dataset_n2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n2 = NewsGroupDataset(test_data_indices_n2, test_labels)\n",
    "test_loader_n2 = torch.utils.data.DataLoader(dataset=test_dataset_n2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n2_ntk = NewsGroupDataset(train_data_indices_n2_ntk, train_labels)\n",
    "train_loader_n2_ntk = torch.utils.data.DataLoader(dataset=train_dataset_n2_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n2_ntk = NewsGroupDataset(val_data_indices_n2_ntk, val_labels)\n",
    "val_loader_n2_ntk = torch.utils.data.DataLoader(dataset=val_dataset_n2_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n2_ntk = NewsGroupDataset(test_data_indices_n2_ntk, test_labels)\n",
    "test_loader_n2_ntk = torch.utils.data.DataLoader(dataset=test_dataset_n2_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n3 = NewsGroupDataset(train_data_indices_n3, train_labels)\n",
    "train_loader_n3 = torch.utils.data.DataLoader(dataset=train_dataset_n3, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n3 = NewsGroupDataset(val_data_indices_n3, val_labels)\n",
    "val_loader_n3 = torch.utils.data.DataLoader(dataset=val_dataset_n3, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n3 = NewsGroupDataset(test_data_indices_n3, test_labels)\n",
    "test_loader_n3 = torch.utils.data.DataLoader(dataset=test_dataset_n3, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n3_ntk = NewsGroupDataset(train_data_indices_n3_ntk, train_labels)\n",
    "train_loader_n3_ntk = torch.utils.data.DataLoader(dataset=train_dataset_n3_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n3_ntk = NewsGroupDataset(val_data_indices_n3_ntk, val_labels)\n",
    "val_loader_n3_ntk = torch.utils.data.DataLoader(dataset=val_dataset_n3_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n3_ntk = NewsGroupDataset(test_data_indices_n3_ntk, test_labels)\n",
    "test_loader_n3_ntk = torch.utils.data.DataLoader(dataset=test_dataset_n3_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n4 = NewsGroupDataset(train_data_indices_n4, train_labels)\n",
    "train_loader_n4 = torch.utils.data.DataLoader(dataset=train_dataset_n4, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n4 = NewsGroupDataset(val_data_indices_n4, val_labels)\n",
    "val_loader_n4 = torch.utils.data.DataLoader(dataset=val_dataset_n4, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n4 = NewsGroupDataset(test_data_indices_n4, test_labels)\n",
    "test_loader_n4 = torch.utils.data.DataLoader(dataset=test_dataset_n4, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n4_ntk = NewsGroupDataset(train_data_indices_n4_ntk, train_labels)\n",
    "train_loader_n4_ntk = torch.utils.data.DataLoader(dataset=train_dataset_n4_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n4_ntk = NewsGroupDataset(val_data_indices_n4_ntk, val_labels)\n",
    "val_loader_n4_ntk = torch.utils.data.DataLoader(dataset=val_dataset_n4_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n4_ntk = NewsGroupDataset(test_data_indices_n4_ntk, test_labels)\n",
    "test_loader_n4_ntk = torch.utils.data.DataLoader(dataset=test_dataset_n4_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of N-gram Model & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BagOfNgram(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfNgram classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNgram, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim, 2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    \n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_proc(model, train_loader, val_loader, lr, adj, ep, optim, lr_decay=0, plt=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if optim == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    if adj:\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=lr_decay)\n",
    "    \n",
    "    train_ls = []\n",
    "    for epoch in range(ep):\n",
    "        if adj:\n",
    "            scheduler.step()\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_ls.append(loss)\n",
    "            \n",
    "#             if i > 0 and i % 100 == 0:\n",
    "#                 val_acc, val_loss = test_model(val_loader, model)\n",
    "#                 val_ls += val_loss\n",
    "#                 print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "#                     epoch+1, ep, i+1, len(train_loader), val_acc))\n",
    "    \n",
    "    val_acc  = test_model(val_loader, model)\n",
    "    print('Val Accuracy: {}'.format(val_acc))\n",
    "    \n",
    "    if plt:\n",
    "        plt.plot(train_ls)\n",
    "        plt.xlabel(\"n\")\n",
    "        plt.ylabel(\"Train Loss\")\n",
    "    \n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.32\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.72\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.14\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 88.06\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.32\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 88.4\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 65.84\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 63.42\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.9\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 62.34\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.1\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 63.18\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.54\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.7\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.88\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 88.42\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.36\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 88.3\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 69.42\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 65.52\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.22\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 64.94\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 67.18\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 67.72\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word) no tokenizaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.12\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.94\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.1\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 88.06\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.66\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.78\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.94\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 61.2\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 67.02\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 60.72\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 65.66\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 62.52\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.7\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.06\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.84\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.12\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.78\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.88\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 69.16\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 63.34\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.2\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 65.3\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 67.4\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.32\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.54\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.6\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.46\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.84\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.08\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.3\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 55.98\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 55.0\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 60.6\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.74\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 56.6\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 57.28\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 82.98\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.04\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 82.6\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.68\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.72\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.38\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 54.34\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.54\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.6\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 59.56\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 57.06\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 60.64\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.04\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.06\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.22\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.1\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.84\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.24\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 51.96\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.1\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 58.16\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.18\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 56.0\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 55.6\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 5, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.2\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.2\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 82.46\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.14\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.34\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.78\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 59.2\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 56.56\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 62.2\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 56.86\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 61.32\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 59.7\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 5, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.18\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.18\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.54\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.44\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 79.74\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 80.44\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.34\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.18\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 54.8\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.56\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.2\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 54.56\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.58\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.02\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.16\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 79.84\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 79.86\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 80.18\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 51.5\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.34\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.32\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.32\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.94\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.6\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.28\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.88\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.62\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 79.22\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 79.92\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.24\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.38\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.2\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.26\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 51.66\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.08\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.9\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.64\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.22\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 79.78\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.94\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 80.14\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 55.02\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.12\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 55.5\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.76\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.82\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.68\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.96\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.46\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.22\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 74.28\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 74.64\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.84\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.36\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.72\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.56\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.06\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.48\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.9\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 74.46\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 74.58\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.84\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.18\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.84\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.36\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 73.48\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.1\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.3\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.52\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 73.34\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 73.28\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.72\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.74\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.26\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.84\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.54\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 51.74\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.18\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.52\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.0\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.4\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.84\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 73.62\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.88\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.18\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.14\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.6\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary (result of the best model on test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_proc(model, train_loader, test_loader, lr, adj, ep, optim, lr_decay=0, pl=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if optim == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    if adj:\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=lr_decay)\n",
    "    \n",
    "    train_ls = []\n",
    "    for epoch in range(ep):\n",
    "        if adj:\n",
    "            scheduler.step()\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train.append(loss)\n",
    "    \n",
    "    test_acc  = test_model(test_loader, model)\n",
    "    print('Test Accuracy: {}'.format(test_acc))\n",
    "    \n",
    "    if pl:\n",
    "        plt.plot(train_ls)\n",
    "        plt.xlabel(\"i\")\n",
    "        plt.ylabel(\"Train Loss\")\n",
    "    \n",
    "    return test_acc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.524\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPst7SwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dba91c4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb_size = 100\n",
    "model_best = BagOfNgram(len(id2token_n1), emb_size)\n",
    "test_acc, model = train_proc(model_best, train_loader_n1, test_loader_n1, 0.01, True, 3, 'Adam', 0.1, pl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "index = []\n",
    "for data, lengths, labels in val_loader_n1:\n",
    "    data_batch, length_batch, label_batch = data, lengths, labels\n",
    "    outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "    predicted = outputs.max(1, keepdim=True)[1]\n",
    "    \n",
    "    index += (predicted.numpy() == labels.view_as(predicted).numpy()).tolist()\n",
    "    \n",
    "correct_index = []\n",
    "wrong_index = []\n",
    "for i, val in enumerate(index):\n",
    "    if val[0] == True:\n",
    "        correct_index.append(i)\n",
    "    else:\n",
    "        wrong_index.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surprisingly good early effort from Alfred Hitchcock. One of the only original screenplays written by Hitchcock himself, this film shows remarkable story structure. It kicks off with a rousing boxing match in which carnival champ \"One Round\" Jack loses to a challenger from the audience who happens to be a professional prizefighter. The movie then slows down to develop the characters and introduce a love triangle between Jack, his girl and the professional boxer. The rest of the film is a dramatic buildup to the rematch between the two men, this time for the heavyweight crown. Even in this early film, Hitchcock shows his talent for meaningful cinematography and prop placement. An armband bought for the girl by the boxer continues to pop up throughout the movie as a symbol of her unfaithfulness. The only big detractor of this film is that the art of filming a boxing match had not yet been perfected in 1927. The final match, as a result, ends up being somewhat anticlimactic. The story, though, is what carries this film through.\n",
      "\n",
      "Not only is this a great African-American classic comedy, but one of many great American cult classics.I have recently purchased the collection edition of Rudy Ray Moore.If you love the old school karate movies and black comedies, this is for you! They don't make movies like these anymore. My entire family are movie buffs, so this site is an extreme help on solving many debates. I am deployed in Iraq right now. This helps me to stay connected to world that I know in the states. Thank you IMDb.I recommend this site to all my friends. Dolemite rules! Don't just take my word for it, check them out for yourself. Ten lines is a lot for commenting on one movie I think, but if it gets the point across, I'm all for it!\n",
      "\n",
      "Having heard so much about the 1990s Cracker series without seeing any of them, I looked forward to this eagerly. Surely the combination of Jimmie McGovern and Robbie Coltrane could not go wrong. How wrong I was! <br /><br />The polemics, backed by frequent, repetitive and violent flashbacks, were overpowering. The production tried to be super-modern, but the flashing boxes and even the childish font irritated. Robbie Coltrane sleep-walked through the two hours, coming up with unexplained and unlikely \"insights\", and the police were portrayed as one-dimensional bumbling idiots. As a result, the tension never built up and the next-to-final scene (no details for fear of spoilers) was as laughably bad a piece of TV drama as I have seen for a long time.<br /><br />No, I don't want to see any more of these, but I will go back to the DVDs of the 1990s series to see if they match their reputation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "three_list = [i for i in np.random.choice(correct_index, 3)]\n",
    "for i in three_list:\n",
    "    print(val_data[i]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm basing this on my observations of one episode I saw last night (9/27/06). I don't think I'll be watching again. The acting was totally wooden, the plot completely predictable, the ending totally unrealistic -- I mean who would believe a 30 million dollar judgment for the death of a recovering drug addict with terminal cancer? The lead actor (Victor Garber) seemed so uncomfortable, almost embarrassed in his role -- perhaps he realized how bad the writing was!! I fully realize that the drama offered this season is pretty poor, but they can surely find better writers. Maybe they are outsourcing the writing to India or China!! I'll bet we won't be seeing this one next season!\n",
      "\n",
      "I LOVE Don Knotts, let me just say that up-front! He is an enormous talent and the best at what he does, which is portray a nervous, lovably befuddled loser thrown into a position of authority. He is fabulous in this role as Roy Fleming, the Reluctant Astronaut, but the film is pretty dull, really, even though as a kid my brother and I delighted in watching this and his other films. It's still worth watching but really it's a film that is best enjoyed by children. I'd categorize it as 100% family-friendly and something you could sit down and watch with your kids on a family night.<br /><br />As with all of Knotts' films, there's a great cast of beloved character actors and you can't help but smile when Knotts gives one of his shaky, open-mouthed stares, no matter how old and jaded you are.<br /><br />From an adult perspective, one thing I think that is great about this film is how it captures NASA in the 1960s -- all the new modern buildings, the hope, the optimism, the future! And I was surprised at how suave and studly Leslie Neilsen was back then. Only complaint about the story is Roy's love interest, a rather threadbare, unlikeable woman who can't give him the time of day until he becomes a big shot -- if you're like me, you'll be hoping that he gives her the shove-off at the end. Beware -- you'll be whistling the theme tune for days after watching, it's that catchy.\n",
      "\n",
      "FINALLY!!!!!!!!!!! I've been waiting for this film to come out for almost a year, and finally saw it at the premiere in SB. I met a few of the actors, who were really nice and who were great in the movie. i watched the trailer so many times that i didn't know what to expect but got totally sucked in. the film was really beautiful to look at it and the music was good too. i recommend it to anyone who's a ryan donowho fan, and dominique swain was good in it too. the other actors were good also great. I hope it comes out on DVD soon!!!!!<br /><br />i first got into ryan from watching the OC and then saw him in a bunch of good indies like Imaginary Heroes. He is great in this film, and everything that he does that's indie. I also like Dominique but haven't seen her in as much. Hope to see them both in more soon!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "three_list = [i for i in np.random.choice(wrong_index, 3)]\n",
    "for i in three_list:\n",
    "    print(val_data[i]+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
