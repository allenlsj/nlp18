{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import pickle as pkl\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(folder_path, label): \n",
    "    scores = []\n",
    "    data_list = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            scores.append(int(file[file.find(\"_\")+1:file.find(\".\")]))\n",
    "            with open(folder_path+file) as f:\n",
    "                data_list.append(f.read())\n",
    "    \n",
    "    labels = label*np.ones(len(scores))\n",
    "    return data_list, labels, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_data(data1, data2, label1, label2, score1, score2, split, shuffle, train_size=20000):\n",
    "    data = data1+data2\n",
    "    labels = np.concatenate([label1, label2]).tolist()\n",
    "    scores = score1+score2\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.seed(0)\n",
    "        index = np.random.permutation(len(data))\n",
    "        data = np.array(data)[index].tolist()\n",
    "        labels = np.array(labels)[index].tolist()\n",
    "        scores = np.array(scores)[index].tolist()\n",
    "    \n",
    "    if split:\n",
    "        train_data = data[:train_size]\n",
    "        val_data = data[train_size:]\n",
    "        train_labels = labels[:train_size]\n",
    "        val_labels = labels[train_size:]\n",
    "        train_scores = scores[:train_size]\n",
    "        val_scores = scores[train_size]\n",
    "        return train_data, train_labels, train_scores, val_data, val_labels, val_scores\n",
    "    \n",
    "    return data, labels, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_path = os.getcwd()+'/aclImdb/train/pos/'\n",
    "train_neg_path = os.getcwd()+'/aclImdb/train/neg/'\n",
    "test_pos_path = os.getcwd()+'/aclImdb/test/pos/'\n",
    "test_neg_path = os.getcwd()+'/aclImdb/test/neg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_data, train_pos_label, train_pos_scores = load_data(train_pos_path, 1)\n",
    "train_neg_data, train_neg_label, train_neg_scores = load_data(train_neg_path, 0)\n",
    "test_pos_data, test_pos_label, test_pos_scores = load_data(test_pos_path, 1)\n",
    "test_neg_data, test_neg_label, test_neg_scores = load_data(test_neg_path, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_data, train_labels, train_scores, \\\n",
    "# val_data, val_labels, val_scores = merge_data(train_pos_data, train_neg_data, train_pos_label, train_neg_label,\n",
    "#                                               train_pos_scores, train_neg_scores, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data, test_labels, test_scores = \\\n",
    "merge_data(test_pos_data, test_neg_data, test_pos_label, test_neg_label,\n",
    "                                              test_pos_scores, test_neg_scores, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pkl.dump(train_data, open(\"train_data.p\", \"wb\"))\n",
    "# pkl.dump(train_labels, open(\"train_labels.p\", \"wb\"))\n",
    "# pkl.dump(train_scores, open(\"train_scores.p\", \"wb\"))\n",
    "# pkl.dump(val_data, open(\"val_data.p\", \"wb\"))\n",
    "# pkl.dump(val_labels, open(\"val_labels.p\", \"wb\"))\n",
    "# pkl.dump(val_scores, open(\"val_scores.p\", \"wb\"))\n",
    "train_data = pkl.load(open(\"train_data.p\", \"rb\"))\n",
    "train_labels = pkl.load(open(\"train_labels.p\", \"rb\"))\n",
    "train_scores = pkl.load(open(\"train_scores.p\", \"rb\"))\n",
    "val_data = pkl.load(open(\"val_data.p\", \"rb\"))\n",
    "val_labels = pkl.load(open(\"val_labels.p\", \"rb\"))\n",
    "val_scores = pkl.load(open(\"val_scores.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def tokenize(sent, tokenization):\n",
    "    tokens = tokenizer(sent)\n",
    "    if tokenization:\n",
    "        return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    else:\n",
    "        return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenization):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample, tokenization)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens, _ = tokenize_dataset(val_data, True)\n",
    "# pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "# print (\"Tokenizing test data\")\n",
    "# test_data_tokens, _ = tokenize_dataset(test_data, True)\n",
    "# pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens, all_train_tokens = tokenize_dataset(train_data, True)\n",
    "# pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4809135\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word) no tokenization (lowercase & remove punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens_ntk, _ = tokenize_dataset(val_data, False)\n",
    "# pkl.dump(val_data_tokens_ntk, open(\"val_data_tokens_ntk.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "# print (\"Tokenizing test data\")\n",
    "# test_data_tokens_ntk, _ = tokenize_dataset(test_data, False)\n",
    "# pkl.dump(test_data_tokens_ntk, open(\"test_data_tokens_ntk.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens_ntk, all_train_tokens_ntk = tokenize_dataset(train_data, False)\n",
    "# pkl.dump(train_data_tokens_ntk, open(\"train_data_tokens_ntk.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens_ntk, open(\"all_train_tokens_ntk.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5439707\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_ntk = pkl.load(open(\"train_data_tokens_ntk.p\", \"rb\"))\n",
    "all_train_tokens_ntk = pkl.load(open(\"all_train_tokens_ntk.p\", \"rb\"))\n",
    "val_data_tokens_ntk = pkl.load(open(\"val_data_tokens_ntk.p\", \"rb\"))\n",
    "test_data_tokens_ntk = pkl.load(open(\"test_data_tokens_ntk.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_ntk)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_dataset_ngram(dataset, n, tokenization):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample, tokenization)\n",
    "        n_grams = list(ngrams(tokens, n))\n",
    "        token_dataset.append(n_grams)\n",
    "        all_tokens += n_grams\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens_n2, _ = tokenize_dataset_ngram(val_data, 2, True)\n",
    "# pkl.dump(val_data_tokens_n2, open(\"val_data_tokens_n2.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "# print (\"Tokenizing test data\")\n",
    "# test_data_tokens_n2, _ = tokenize_dataset_ngram(test_data, 2, True)\n",
    "# pkl.dump(test_data_tokens_n2, open(\"test_data_tokens_n2.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens_n2, all_train_tokens_n2 = tokenize_dataset_ngram(train_data, 2, True)\n",
    "# pkl.dump(train_data_tokens_n2, open(\"train_data_tokens_n2.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens_n2, open(\"all_train_tokens_n2.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4789135\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n2 = pkl.load(open(\"train_data_tokens_n2.p\", \"rb\"))\n",
    "all_train_tokens_n2 = pkl.load(open(\"all_train_tokens_n2.p\", \"rb\"))\n",
    "val_data_tokens_n2 = pkl.load(open(\"val_data_tokens_n2.p\", \"rb\"))\n",
    "test_data_tokens_n2 = pkl.load(open(\"test_data_tokens_n2.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n2)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n2)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n2)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams no tokenization (lowercase & remove punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens_n2_ntk, _ = tokenize_dataset_ngram(val_data, 2, False)\n",
    "# pkl.dump(val_data_tokens_n2_ntk, open(\"val_data_tokens_n2_ntk.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "# print (\"Tokenizing test data\")\n",
    "# test_data_tokens_n2_ntk, _ = tokenize_dataset_ngram(test_data, 2, False)\n",
    "# pkl.dump(test_data_tokens_n2_ntk, open(\"test_data_tokens_n2_ntk.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens_n2_ntk, all_train_tokens_n2_ntk = tokenize_dataset_ngram(train_data, 2, False)\n",
    "# pkl.dump(train_data_tokens_n2_ntk, open(\"train_data_tokens_n2_ntk.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens_n2_ntk, open(\"all_train_tokens_n2_ntk.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5419707\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n2_ntk = pkl.load(open(\"train_data_tokens_n2_ntk.p\", \"rb\"))\n",
    "all_train_tokens_n2_ntk = pkl.load(open(\"all_train_tokens_n2_ntk.p\", \"rb\"))\n",
    "val_data_tokens_n2_ntk = pkl.load(open(\"val_data_tokens_n2_ntk.p\", \"rb\"))\n",
    "test_data_tokens_n2_ntk = pkl.load(open(\"test_data_tokens_n2_ntk.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n2_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n2_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n2_ntk)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n2_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens_n3, _ = tokenize_dataset_ngram(val_data, 3, True)\n",
    "# pkl.dump(val_data_tokens_n3, open(\"val_data_tokens_n3.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "# print (\"Tokenizing test data\")\n",
    "# test_data_tokens_n3, _ = tokenize_dataset_ngram(test_data, 3, True)\n",
    "# pkl.dump(test_data_tokens_n3, open(\"test_data_tokens_n3.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens_n3, all_train_tokens_n3 = tokenize_dataset_ngram(train_data, 3, True)\n",
    "# pkl.dump(train_data_tokens_n3, open(\"train_data_tokens_n3.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens_n3, open(\"all_train_tokens_n3.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4769135\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n3 = pkl.load(open(\"train_data_tokens_n3.p\", \"rb\"))\n",
    "all_train_tokens_n3 = pkl.load(open(\"all_train_tokens_n3.p\", \"rb\"))\n",
    "val_data_tokens_n3 = pkl.load(open(\"val_data_tokens_n3.p\", \"rb\"))\n",
    "test_data_tokens_n3 = pkl.load(open(\"test_data_tokens_n3.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n3)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n3)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n3)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams no tokenization (lowercase & remove punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens_n3_ntk, _ = tokenize_dataset_ngram(val_data, 3, False)\n",
    "# pkl.dump(val_data_tokens_n3_ntk, open(\"val_data_tokens_n3_ntk.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "# print (\"Tokenizing test data\")\n",
    "# test_data_tokens_n3_ntk, _ = tokenize_dataset_ngram(test_data, 3, False)\n",
    "# pkl.dump(test_data_tokens_n3_ntk, open(\"test_data_tokens_n3_ntk.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens_n3_ntk, all_train_tokens_n3_ntk = tokenize_dataset_ngram(train_data, 3, False)\n",
    "# pkl.dump(train_data_tokens_n3_ntk, open(\"train_data_tokens_n3_ntk.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens_n3_ntk, open(\"all_train_tokens_n3_ntk.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5399707\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n3_ntk = pkl.load(open(\"train_data_tokens_n3_ntk.p\", \"rb\"))\n",
    "all_train_tokens_n3_ntk = pkl.load(open(\"all_train_tokens_n3_ntk.p\", \"rb\"))\n",
    "val_data_tokens_n3_ntk = pkl.load(open(\"val_data_tokens_n3_ntk.p\", \"rb\"))\n",
    "test_data_tokens_n3_ntk = pkl.load(open(\"test_data_tokens_n3_ntk.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n3_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n3_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n3_ntk)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n3_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens_n4, _ = tokenize_dataset_ngram(val_data, 4, True)\n",
    "# pkl.dump(val_data_tokens_n4, open(\"val_data_tokens_n4.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "# print (\"Tokenizing test data\")\n",
    "# test_data_tokens_n4, _ = tokenize_dataset_ngram(test_data, 4, True)\n",
    "# pkl.dump(test_data_tokens_n4, open(\"test_data_tokens_n4.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens_n4, all_train_tokens_n4 = tokenize_dataset_ngram(train_data, 4, True)\n",
    "# pkl.dump(train_data_tokens_n4, open(\"train_data_tokens_n4.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens_n4, open(\"all_train_tokens_n4.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4749135\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n4 = pkl.load(open(\"train_data_tokens_n4.p\", \"rb\"))\n",
    "all_train_tokens_n4 = pkl.load(open(\"all_train_tokens_n4.p\", \"rb\"))\n",
    "val_data_tokens_n4 = pkl.load(open(\"val_data_tokens_n4.p\", \"rb\"))\n",
    "test_data_tokens_n4 = pkl.load(open(\"test_data_tokens_n4.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n4)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n4)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n4)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams no tokenization (lowercase & remove punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens_n4_ntk, _ = tokenize_dataset_ngram(val_data, 4, False)\n",
    "# pkl.dump(val_data_tokens_n4_ntk, open(\"val_data_tokens_n4_ntk.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "# print (\"Tokenizing test data\")\n",
    "# test_data_tokens_n4_ntk, _ = tokenize_dataset_ngram(test_data, 4, False)\n",
    "# pkl.dump(test_data_tokens_n4_ntk, open(\"test_data_tokens_n4_ntk.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens_n4_ntk, all_train_tokens_n4_ntk = tokenize_dataset_ngram(train_data, 4, False)\n",
    "# pkl.dump(train_data_tokens_n4_ntk, open(\"train_data_tokens_n4_ntk.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens_n4_ntk, open(\"all_train_tokens_n4_ntk.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5379707\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens_n4_ntk = pkl.load(open(\"train_data_tokens_n4_ntk.p\", \"rb\"))\n",
    "all_train_tokens_n4_ntk = pkl.load(open(\"all_train_tokens_n4_ntk.p\", \"rb\"))\n",
    "val_data_tokens_n4_ntk = pkl.load(open(\"val_data_tokens_n4_ntk.p\", \"rb\"))\n",
    "test_data_tokens_n4_ntk = pkl.load(open(\"test_data_tokens_n4_ntk.p\", \"rb\"))\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens_n4_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens_n4_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens_n4_ntk)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_n4_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab lists and transform data into indices lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "token2id_n1, id2token_n1 = build_vocab(all_train_tokens, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n1 = token2index_dataset(train_data_tokens, token2id_n1)\n",
    "val_data_indices_n1 = token2index_dataset(val_data_tokens, token2id_n1)\n",
    "test_data_indices_n1 = token2index_dataset(test_data_tokens, token2id_n1)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n1)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n1)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word) no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "token2id_n1_ntk, id2token_n1_ntk = build_vocab(all_train_tokens_ntk, max_vocab_size = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n1_ntk = token2index_dataset(train_data_tokens_ntk, token2id_n1_ntk)\n",
    "val_data_indices_n1_ntk = token2index_dataset(val_data_tokens_ntk, token2id_n1_ntk)\n",
    "test_data_indices_n1_ntk = token2index_dataset(test_data_tokens_ntk, token2id_n1_ntk)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n1_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n1_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n1_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "token2id_n2, id2token_n2 = build_vocab(all_train_tokens_n2, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n2 = token2index_dataset(train_data_tokens_n2, token2id_n2)\n",
    "val_data_indices_n2 = token2index_dataset(val_data_tokens_n2, token2id_n2)\n",
    "test_data_indices_n2 = token2index_dataset(test_data_tokens_n2, token2id_n2)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n2)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n2)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "token2id_n2_ntk, id2token_n2_ntk = build_vocab(all_train_tokens_n2_ntk, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n2_ntk = token2index_dataset(train_data_tokens_n2_ntk, token2id_n2_ntk)\n",
    "val_data_indices_n2_ntk = token2index_dataset(val_data_tokens_n2_ntk, token2id_n2_ntk)\n",
    "test_data_indices_n2_ntk = token2index_dataset(test_data_tokens_n2_ntk, token2id_n2_ntk)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n2_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n2_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n2_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "token2id_n3, id2token_n3 = build_vocab(all_train_tokens_n3, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n3 = token2index_dataset(train_data_tokens_n3, token2id_n3)\n",
    "val_data_indices_n3 = token2index_dataset(val_data_tokens_n3, token2id_n3)\n",
    "test_data_indices_n3 = token2index_dataset(test_data_tokens_n3, token2id_n3)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n3)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n3)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "token2id_n3_ntk, id2token_n3_ntk = build_vocab(all_train_tokens_n3_ntk, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n3_ntk = token2index_dataset(train_data_tokens_n3_ntk, token2id_n3_ntk)\n",
    "val_data_indices_n3_ntk = token2index_dataset(val_data_tokens_n3_ntk, token2id_n3_ntk)\n",
    "test_data_indices_n3_ntk = token2index_dataset(test_data_tokens_n3_ntk, token2id_n3_ntk)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n3_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n3_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n3_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "token2id_n4, id2token_n4 = build_vocab(all_train_tokens_n4, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n4 = token2index_dataset(train_data_tokens_n4, token2id_n4)\n",
    "val_data_indices_n4 = token2index_dataset(val_data_tokens_n4, token2id_n4)\n",
    "test_data_indices_n4 = token2index_dataset(test_data_tokens_n4, token2id_n4)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n4)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n4)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "token2id_n4_ntk, id2token_n4_ntk = build_vocab(all_train_tokens_n4_ntk, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_n4_ntk = token2index_dataset(train_data_tokens_n4_ntk, token2id_n4_ntk)\n",
    "val_data_indices_n4_ntk = token2index_dataset(val_data_tokens_n4_ntk, token2id_n4_ntk)\n",
    "test_data_indices_n4_ntk = token2index_dataset(test_data_tokens_n4_ntk, token2id_n4_ntk)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_n4_ntk)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices_n4_ntk)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_n4_ntk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when yo-u call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1-gram (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n1 = NewsGroupDataset(train_data_indices_n1, train_labels)\n",
    "train_loader_n1 = torch.utils.data.DataLoader(dataset=train_dataset_n1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n1 = NewsGroupDataset(val_data_indices_n1, val_labels)\n",
    "val_loader_n1 = torch.utils.data.DataLoader(dataset=val_dataset_n1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n1 = NewsGroupDataset(test_data_indices_n1, test_labels)\n",
    "test_loader_n1 = torch.utils.data.DataLoader(dataset=test_dataset_n1, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word) no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n1_ntk = NewsGroupDataset(train_data_indices_n1_ntk, train_labels)\n",
    "train_loader_n1_ntk = torch.utils.data.DataLoader(dataset=train_dataset_n1_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n1_ntk = NewsGroupDataset(val_data_indices_n1_ntk, val_labels)\n",
    "val_loader_n1_ntk = torch.utils.data.DataLoader(dataset=val_dataset_n1_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n1_ntk = NewsGroupDataset(test_data_indices_n1_ntk, test_labels)\n",
    "test_loader_n1_ntk = torch.utils.data.DataLoader(dataset=test_dataset_n1_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n2 = NewsGroupDataset(train_data_indices_n2, train_labels)\n",
    "train_loader_n2 = torch.utils.data.DataLoader(dataset=train_dataset_n2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n2 = NewsGroupDataset(val_data_indices_n2, val_labels)\n",
    "val_loader_n2 = torch.utils.data.DataLoader(dataset=val_dataset_n2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n2 = NewsGroupDataset(test_data_indices_n2, test_labels)\n",
    "test_loader_n2 = torch.utils.data.DataLoader(dataset=test_dataset_n2, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n2_ntk = NewsGroupDataset(train_data_indices_n2_ntk, train_labels)\n",
    "train_loader_n2_ntk = torch.utils.data.DataLoader(dataset=train_dataset_n2_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n2_ntk = NewsGroupDataset(val_data_indices_n2_ntk, val_labels)\n",
    "val_loader_n2_ntk = torch.utils.data.DataLoader(dataset=val_dataset_n2_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n2_ntk = NewsGroupDataset(test_data_indices_n2_ntk, test_labels)\n",
    "test_loader_n2_ntk = torch.utils.data.DataLoader(dataset=test_dataset_n2_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n3 = NewsGroupDataset(train_data_indices_n3, train_labels)\n",
    "train_loader_n3 = torch.utils.data.DataLoader(dataset=train_dataset_n3, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n3 = NewsGroupDataset(val_data_indices_n3, val_labels)\n",
    "val_loader_n3 = torch.utils.data.DataLoader(dataset=val_dataset_n3, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n3 = NewsGroupDataset(test_data_indices_n3, test_labels)\n",
    "test_loader_n3 = torch.utils.data.DataLoader(dataset=test_dataset_n3, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n3_ntk = NewsGroupDataset(train_data_indices_n3_ntk, train_labels)\n",
    "train_loader_n3_ntk = torch.utils.data.DataLoader(dataset=train_dataset_n3_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n3_ntk = NewsGroupDataset(val_data_indices_n3_ntk, val_labels)\n",
    "val_loader_n3_ntk = torch.utils.data.DataLoader(dataset=val_dataset_n3_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n3_ntk = NewsGroupDataset(test_data_indices_n3_ntk, test_labels)\n",
    "test_loader_n3_ntk = torch.utils.data.DataLoader(dataset=test_dataset_n3_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n4 = NewsGroupDataset(train_data_indices_n4, train_labels)\n",
    "train_loader_n4 = torch.utils.data.DataLoader(dataset=train_dataset_n4, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n4 = NewsGroupDataset(val_data_indices_n4, val_labels)\n",
    "val_loader_n4 = torch.utils.data.DataLoader(dataset=val_dataset_n4, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n4 = NewsGroupDataset(test_data_indices_n4, test_labels)\n",
    "test_loader_n4 = torch.utils.data.DataLoader(dataset=test_dataset_n4, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset_n4_ntk = NewsGroupDataset(train_data_indices_n4_ntk, train_labels)\n",
    "train_loader_n4_ntk = torch.utils.data.DataLoader(dataset=train_dataset_n4_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset_n4_ntk = NewsGroupDataset(val_data_indices_n4_ntk, val_labels)\n",
    "val_loader_n4_ntk = torch.utils.data.DataLoader(dataset=val_dataset_n4_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset_n4_ntk = NewsGroupDataset(test_data_indices_n4_ntk, test_labels)\n",
    "test_loader_n4_ntk = torch.utils.data.DataLoader(dataset=test_dataset_n4_ntk, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of N-gram Models & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BagOfNgram(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfNgram classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNgram, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim, 2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    \n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_proc(model, train_loader, val_loader, lr, adj, ep, optim, lr_decay=0, plt=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if optim == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    if adj:\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=lr_decay)\n",
    "    \n",
    "    train_ls = []\n",
    "    for epoch in range(ep):\n",
    "        if adj:\n",
    "            scheduler.step()\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_ls.append(loss)\n",
    "            \n",
    "#             if i > 0 and i % 300 == 0:\n",
    "#                 val_acc, val_loss = test_model(val_loader, model)\n",
    "#                 val_ls += val_loss\n",
    "#                 print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "#                     epoch+1, ep, i+1, len(train_loader), val_acc))\n",
    "    \n",
    "    val_acc = test_model(val_loader, model)\n",
    "    print('Val Accuracy: {}'.format(val_acc))\n",
    "    \n",
    "    if plt:\n",
    "        plt.plot(train_ls)\n",
    "        plt.xlabel(\"n\")\n",
    "        plt.ylabel(\"Train Loss\")\n",
    "    \n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.8\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.26\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.84\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.3\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.82\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.72\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.1\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 61.14\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 67.28\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 61.08\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 64.4\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.32\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.62\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.12\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.02\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 88.04\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.62\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.8\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 69.72\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 63.36\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.9\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 65.44\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 69.16\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.9\n"
     ]
    }
   ],
   "source": [
    "model_n1 = BagOfNgram(len(id2token_n1), emb_size)\n",
    "val_acc = train_proc(model_n1, train_loader_n1, val_loader_n1,  0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram (word) no tokenizaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.48\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.66\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.52\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.86\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.86\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.44\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.76\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 60.68\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 68.2\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 59.56\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.04\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 65.0\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 85.36\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.28\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 84.72\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.4\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 86.58\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 87.26\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 69.56\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 61.74\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.04\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 64.42\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 67.94\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk,  0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 67.54\n"
     ]
    }
   ],
   "source": [
    "model_n1_ntk = BagOfNgram(len(id2token_n1_ntk), emb_size)\n",
    "val_acc = train_proc(model_n1_ntk, train_loader_n1_ntk, val_loader_n1_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 82.0\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 82.04\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 80.48\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.84\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.38\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.78\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 56.0\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.64\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 56.16\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 54.7\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 58.82\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 58.44\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 80.18\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 82.72\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 79.9\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.68\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.04\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.76\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.22\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.58\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 61.96\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 58.62\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 55.06\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 61.92\n"
     ]
    }
   ],
   "source": [
    "model_n2 = BagOfNgram(len(id2token_n2), emb_size)\n",
    "val_acc = train_proc(model_n2, train_loader_n2, val_loader_n2, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 81.04\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 81.86\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 81.04\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.92\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.16\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.2\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 54.38\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.66\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.14\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.18\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 51.22\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 57.34\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 5, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 81.2\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 82.78\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 80.24\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.72\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 82.48\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 83.54\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.72\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 57.7\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 57.64\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.88\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 54.4\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 58.46\n"
     ]
    }
   ],
   "source": [
    "model_n2_ntk = BagOfNgram(len(id2token_n2_ntk), emb_size)\n",
    "val_acc = train_proc(model_n2_ntk, train_loader_n2_ntk, val_loader_n2_ntk, 0.01, True, 5, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.94\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 74.66\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.22\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.98\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.74\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.52\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.5\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.86\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.26\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.46\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.86\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 51.22\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.72\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 74.58\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 75.66\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.82\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.9\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 78.24\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 58.22\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.4\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 56.98\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.44\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 57.56\n"
     ]
    }
   ],
   "source": [
    "model_n3 = BagOfNgram(len(id2token_n3), emb_size)\n",
    "val_acc = train_proc(model_n3, train_loader_n3, val_loader_n3, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 75.86\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 69.24\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 74.0\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.48\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.22\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.92\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.04\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.66\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.46\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 54.5\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 54.04\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 75.46\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 74.96\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 74.68\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.16\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 76.44\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 77.66\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.18\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.82\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.2\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 56.7\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.08\n"
     ]
    }
   ],
   "source": [
    "model_n3_ntk = BagOfNgram(len(id2token_n3_ntk), emb_size)\n",
    "val_acc = train_proc(model_n3_ntk, train_loader_n3_ntk, val_loader_n3_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.64\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 67.22\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 69.62\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 66.42\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.48\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.04\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.18\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.88\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.84\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 53.56\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.18\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.38\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 68.54\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.92\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.32\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 72.24\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.74\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.42\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.88\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.18\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 51.66\n"
     ]
    }
   ],
   "source": [
    "model_n4 = BagOfNgram(len(id2token_n4), emb_size)\n",
    "val_acc = train_proc(model_n4, train_loader_n4, val_loader_n4, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-grams no tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.5\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 58.5\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 69.88\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 65.74\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.46\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.64\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.16\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.86\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.88\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.22\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.84\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.92\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.4\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 56.0\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 3, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 69.48\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 65.86\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 5, 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 70.64\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'Adam', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 71.04\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'Adam', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 50.18\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.82\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 3, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.84\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.86\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.001, False, 5, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 49.86\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'SGD', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 52.16\n"
     ]
    }
   ],
   "source": [
    "model_n4_ntk = BagOfNgram(len(id2token_n4_ntk), emb_size)\n",
    "val_acc = train_proc(model_n4_ntk, train_loader_n4_ntk, val_loader_n4_ntk, 0.01, True, 3, 'SGD', 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
