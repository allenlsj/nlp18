{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CNN/RNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_size1, hidden_size2, hidden_size3, num_layers, num_classes, emb_size=300):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size1, self.hidden_size2, self.hidden_size3 = \\\n",
    "        num_layers, hidden_size1, hidden_size2, hidden_size3\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft)).float()\n",
    "        self.rnn1 = nn.GRU(emb_size, hidden_size1, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.rnn2 = nn.GRU(emb_size, hidden_size2, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(hidden_size1+hidden_size2, hidden_size3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size3, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size, hidden_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, hidden_size)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x1, lengths1, x2, lengths2):\n",
    "        batch_size1, seq_len1 = x1.size()\n",
    "        batch_size2, seq_len2 = x2.size()\n",
    "        \n",
    "        _, idx_sort1 = torch.sort(lengths1, dim=0, descending=True)\n",
    "        _, idx_unsort1 = torch.sort(idx_sort1, dim=0)\n",
    "        _, idx_sort2 = torch.sort(lengths2, dim=0, descending=True)\n",
    "        _, idx_unsort2 = torch.sort(idx_sort2, dim=0)\n",
    "        \n",
    "        # reset hidden state\n",
    "        self.hidden1 = self.init_hidden(batch_size1, self.hidden_size1)\n",
    "        self.hidden2 = self.init_hidden(batch_size2, self.hidden_size2)\n",
    "\n",
    "        # get embedding of characters\n",
    "        embed1 = self.embedding(x1)\n",
    "        embed2 = self.embedding(x2)\n",
    "        # pack padded sequence\n",
    "        embed1 = embed1.index_select(0, idx_sort1)\n",
    "        embed1 = torch.nn.utils.rnn.pack_padded_sequence(embed1, lengths1.index_select(0, idx_sort1).numpy(), batch_first=True)\n",
    "        embed2 = embed2.index_select(0, idx_sort2)\n",
    "        embed2 = torch.nn.utils.rnn.pack_padded_sequence(embed2, lengths2.index_select(0, idx_sort2).numpy(), batch_first=True)\n",
    "        # fprop though RNN\n",
    "        rnn_out1, self.hidden1 = self.rnn1(embed1, self.hidden1)\n",
    "        self.hidden1 = self.hidden1.index_select(1, idx_unsort1)\n",
    "        rnn_out2, self.hidden2 = self.rnn2(embed2, self.hidden2)\n",
    "        self.hidden2 = self.hidden2.index_select(1, idx_unsort2)\n",
    "    \n",
    "        combined_vector = torch.cat([self.hidden1, self.hidden2],dim=-1)\n",
    "        rnn_out = torch.sum(combined_vector, dim=0)\n",
    "\n",
    "        fc_out = self.linear1(rnn_out)\n",
    "        fc_out = self.relu(fc_out)\n",
    "        logits = self.linear2(fc_out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, kernel_pad, hidden_size1, hidden_size2, num_layers, num_classes, emb_size=300):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size1, self.hidden_size2 = \\\n",
    "        num_layers, hidden_size1, hidden_size2\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft)).float()\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size1, kernel_size=kernel_pad[0], padding=kernel_pad[1])\n",
    "        self.conv2 = nn.Conv1d(hidden_size1, hidden_size1, kernel_size=kernel_pad[0], padding=kernel_pad[1])\n",
    "        self.conv3 = nn.Conv1d(emb_size, hidden_size1, kernel_size=kernel_pad[0], padding=kernel_pad[1])\n",
    "        self.conv4 = nn.Conv1d(hidden_size1, hidden_size1, kernel_size=kernel_pad[0], padding=kernel_pad[1])\n",
    "        \n",
    "        self.maxpool1 = nn.MaxPool1d(MAX_SENTENCE_LENGTH_X1)\n",
    "        self.maxpool2 = nn.MaxPool1d(MAX_SENTENCE_LENGTH_X2)\n",
    "\n",
    "        self.linear1 = nn.Linear(2*hidden_size1, hidden_size2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size2, num_classes)\n",
    "\n",
    "    def forward(self, x1, lengths1, x2, lengths2):\n",
    "        batch_size1, seq_len1 = x1.size()\n",
    "        batch_size2, seq_len2 = x2.size()\n",
    "\n",
    "        embed1 = self.embedding(x1)\n",
    "        embed2 = self.embedding(x2)\n",
    "        \n",
    "        hidden1 = self.conv1(embed1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size1, seq_len1, hidden1.size(-1))\n",
    "        hidden1 = self.conv2(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size1, hidden1.size(-1), seq_len1)\n",
    "        hidden1 = self.maxpool1(hidden1)\n",
    "        \n",
    "        hidden2 = self.conv3(embed2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, seq_len2, hidden2.size(-1))\n",
    "        hidden2 = self.conv4(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size2, hidden2.size(-1), seq_len2)   \n",
    "        hidden2 = self.maxpool2(hidden2)\n",
    "        \n",
    "        combined_vector = torch.cat([hidden1, hidden2],dim=1)\n",
    "        cnn_out = torch.sum(combined_vector, dim=-1)\n",
    "        \n",
    "        fc_out = self.linear1(cnn_out)\n",
    "        fc_out = self.relu(fc_out)\n",
    "        logits = self.linear2(fc_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best trained RNN/CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_rnn = torch.load('best_snli_model_rnn.pth')\n",
    "model_cnn = torch.load('best_snli_model_cnn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-tokenized val set by genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    y = []\n",
    "    genres = []\n",
    "    with open(filepath) as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            if row == ['sentence1', 'sentence2', 'label', 'genre']:\n",
    "                pass\n",
    "            else:\n",
    "                x1.append(row[0].split())\n",
    "                x2.append(row[1].split())\n",
    "                genres.append(row[3])\n",
    "                if row[2] == 'contradiction':\n",
    "                    y.append(0.0)\n",
    "                elif row[2] == 'entailment':\n",
    "                    y.append(1.0)\n",
    "                elif row[2] == 'neutral':\n",
    "                    y.append(2.0)\n",
    "    \n",
    "    return x1,x2,y,genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_x1, val_x2, val_y, val_genre = load_data('hw2_data/mnli_val.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiction', 'government', 'slate', 'telephone', 'travel'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(val_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH_X1 = 34\n",
    "MAX_SENTENCE_LENGTH_X2 = 19\n",
    "words_to_load = 60000\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load+2, 300))\n",
    "    np.random.seed(1)\n",
    "    loaded_embeddings_ft[UNK_IDX] = np.random.rand(300)\n",
    "    token2id = {'<pad>':PAD_IDX, '<unk>':UNK_IDX}\n",
    "    id2token = {PAD_IDX:'<pad>', UNK_IDX:'<unk>'}\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "        token2id[s[0]] = i+2\n",
    "        id2token[i+2] = s[0]\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list1, data_list2, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of SNLI tokens \n",
    "        @param target_list: list of SNLI targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list1) == len(self.target_list))\n",
    "        assert (len(self.data_list2) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when yo-u call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH_X1]\n",
    "        token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH_X2]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx1, len(token_idx1), token_idx2, len(token_idx2), label]\n",
    "\n",
    "def SNLI_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    label_list = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list1.append(datum[1])\n",
    "        length_list2.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH_X1-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec2 = np.pad(np.array(datum[2]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH_X2-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        data_list2.append(padded_vec2)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(data_list1)), torch.LongTensor(length_list1), \n",
    "            torch.from_numpy(np.array(data_list2)), torch.LongTensor(length_list2),torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1, lengths1, data2, lengths2, labels in loader:\n",
    "        data_batch1, lengths_batch1, data_batch2, lengths_batch2, label_batch = \\\n",
    "        data1, lengths1, data2, lengths2, labels\n",
    "        outputs = F.softmax(model(data_batch1, lengths_batch1, data_batch2, lengths_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_x1_fict = [val for ind, val in enumerate(val_x1) if val_genre[ind]=='fiction']\n",
    "val_x2_fict = [val for ind, val in enumerate(val_x2) if val_genre[ind]=='fiction']\n",
    "val_y_fict = [val for ind, val in enumerate(val_y) if val_genre[ind]=='fiction']\n",
    "val_x1_fict_indices = token2index_dataset(val_x1_fict)\n",
    "val_x2_fict_indices = token2index_dataset(val_x2_fict)\n",
    "val_dataset_fict = SNLIDataset(val_x1_fict_indices, val_x2_fict_indices, val_y_fict)\n",
    "val_loader_fict = torch.utils.data.DataLoader(dataset=val_dataset_fict, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc of best RNN on MNLI(fiction):43.015075376884425\n",
      "Val acc of best CNN on MNLI(fiction): 44.02010050251256\n"
     ]
    }
   ],
   "source": [
    "print(\"Val acc of best RNN on MNLI(fiction):{}\".format(test_model(val_loader_fict, model_rnn)))\n",
    "print(\"Val acc of best CNN on MNLI(fiction): {}\".format(test_model(val_loader_fict, model_cnn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### government"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_x1_gov = [val for ind, val in enumerate(val_x1) if val_genre[ind]=='government']\n",
    "val_x2_gov = [val for ind, val in enumerate(val_x2) if val_genre[ind]=='government']\n",
    "val_y_gov = [val for ind, val in enumerate(val_y) if val_genre[ind]=='government']\n",
    "val_x1_gov_indices = token2index_dataset(val_x1_gov)\n",
    "val_x2_gov_indices = token2index_dataset(val_x2_gov)\n",
    "val_dataset_gov = SNLIDataset(val_x1_gov_indices, val_x2_gov_indices, val_y_gov)\n",
    "val_loader_gov = torch.utils.data.DataLoader(dataset=val_dataset_gov, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc of best RNN on MNLI(government):45.17716535433071\n",
      "Val acc of best CNN on MNLI(government): 41.338582677165356\n"
     ]
    }
   ],
   "source": [
    "print(\"Val acc of best RNN on MNLI(government):{}\".format(test_model(val_loader_gov, model_rnn)))\n",
    "print(\"Val acc of best CNN on MNLI(government): {}\".format(test_model(val_loader_gov, model_cnn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_x1_sla = [val for ind, val in enumerate(val_x1) if val_genre[ind]=='slate']\n",
    "val_x2_sla = [val for ind, val in enumerate(val_x2) if val_genre[ind]=='slate']\n",
    "val_y_sla = [val for ind, val in enumerate(val_y) if val_genre[ind]=='slate']\n",
    "val_x1_sla_indices = token2index_dataset(val_x1_sla)\n",
    "val_x2_sla_indices = token2index_dataset(val_x2_sla)\n",
    "val_dataset_sla = SNLIDataset(val_x1_sla_indices, val_x2_sla_indices, val_y_sla)\n",
    "val_loader_sla = torch.utils.data.DataLoader(dataset=val_dataset_sla, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc of best RNN on MNLI(slate):41.417165668662676\n",
      "Val acc of best CNN on MNLI(slate): 41.417165668662676\n"
     ]
    }
   ],
   "source": [
    "print(\"Val acc of best RNN on MNLI(slate):{}\".format(test_model(val_loader_sla, model_rnn)))\n",
    "print(\"Val acc of best CNN on MNLI(slate): {}\".format(test_model(val_loader_sla, model_cnn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### telephone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_x1_tel = [val for ind, val in enumerate(val_x1) if val_genre[ind]=='telephone']\n",
    "val_x2_tel = [val for ind, val in enumerate(val_x2) if val_genre[ind]=='telephone']\n",
    "val_y_tel = [val for ind, val in enumerate(val_y) if val_genre[ind]=='telephone']\n",
    "val_x1_tel_indices = token2index_dataset(val_x1_tel)\n",
    "val_x2_tel_indices = token2index_dataset(val_x2_tel)\n",
    "val_dataset_tel = SNLIDataset(val_x1_tel_indices, val_x2_tel_indices, val_y_tel)\n",
    "val_loader_tel = torch.utils.data.DataLoader(dataset=val_dataset_tel, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc of best RNN on MNLI(telephone):47.36318407960199\n",
      "Val acc of best CNN on MNLI(telephone): 43.681592039801\n"
     ]
    }
   ],
   "source": [
    "print(\"Val acc of best RNN on MNLI(telephone):{}\".format(test_model(val_loader_tel, model_rnn)))\n",
    "print(\"Val acc of best CNN on MNLI(telephone): {}\".format(test_model(val_loader_tel, model_cnn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_x1_tra = [val for ind, val in enumerate(val_x1) if val_genre[ind]=='travel']\n",
    "val_x2_tra = [val for ind, val in enumerate(val_x2) if val_genre[ind]=='travel']\n",
    "val_y_tra = [val for ind, val in enumerate(val_y) if val_genre[ind]=='travel']\n",
    "val_x1_tra_indices = token2index_dataset(val_x1_tra)\n",
    "val_x2_tra_indices = token2index_dataset(val_x2_tra)\n",
    "val_dataset_tra = SNLIDataset(val_x1_tra_indices, val_x2_tra_indices, val_y_tra)\n",
    "val_loader_tra = torch.utils.data.DataLoader(dataset=val_dataset_tra, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc of best RNN on MNLI(travel):45.010183299389\n",
      "Val acc of best CNN on MNLI(travel): 42.36252545824847\n"
     ]
    }
   ],
   "source": [
    "print(\"Val acc of best RNN on MNLI(travel):{}\".format(test_model(val_loader_tra, model_rnn)))\n",
    "print(\"Val acc of best CNN on MNLI(travel): {}\".format(test_model(val_loader_tra, model_cnn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning on MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fine_tune_model(model, train_loader, val_loader, learning_rate, num_epochs, adj, lr_decay=0.5):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print(\"Number of trainable parameters:{}\".format(params))\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if adj:\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=lr_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if adj:\n",
    "            scheduler.step()\n",
    "        for i, (data1, lengths1, data2, lengths2, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data1, lengths1, data2, lengths2)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    val_acc_last = test_model(val_loader, model)\n",
    "    print(\"Val Accuracy:{}\".format(val_acc_last))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x1, train_x2, train_y, train_genre = load_data('hw2_data/mnli_train.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiction', 'government', 'slate', 'telephone', 'travel'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x1_fict = [val for ind, val in enumerate(train_x1) if train_genre[ind]=='fiction']\n",
    "train_x2_fict = [val for ind, val in enumerate(train_x2) if train_genre[ind]=='fiction']\n",
    "train_y_fict = [val for ind, val in enumerate(train_y) if train_genre[ind]=='fiction']\n",
    "train_x1_fict_indices = token2index_dataset(train_x1_fict)\n",
    "train_x2_fict_indices = token2index_dataset(train_x2_fict)\n",
    "train_dataset_fict = SNLIDataset(train_x1_fict_indices, train_x2_fict_indices, train_y_fict)\n",
    "train_loader_fict = torch.utils.data.DataLoader(dataset=train_dataset_fict, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters:492603\n",
      "Val Accuracy:44.321608040201006\n"
     ]
    }
   ],
   "source": [
    "model_rnn = torch.load('best_snli_model_rnn.pth')\n",
    "fine_tune_model(model_rnn, train_loader_fict, val_loader_fict, 3e-4, 5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### government"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x1_gov = [val for ind, val in enumerate(train_x1) if train_genre[ind]=='government']\n",
    "train_x2_gov = [val for ind, val in enumerate(train_x2) if train_genre[ind]=='government']\n",
    "train_y_gov = [val for ind, val in enumerate(train_y) if train_genre[ind]=='government']\n",
    "train_x1_gov_indices = token2index_dataset(train_x1_gov)\n",
    "train_x2_gov_indices = token2index_dataset(train_x2_gov)\n",
    "train_dataset_gov = SNLIDataset(train_x1_gov_indices, train_x2_gov_indices, train_y_gov)\n",
    "train_loader_gov = torch.utils.data.DataLoader(dataset=train_dataset_gov, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters:492603\n",
      "Val Accuracy:48.91732283464567\n"
     ]
    }
   ],
   "source": [
    "model_rnn = torch.load('best_snli_model_rnn.pth')\n",
    "fine_tune_model(model_rnn, train_loader_gov, val_loader_gov, 3e-4, 5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x1_sla = [val for ind, val in enumerate(train_x1) if train_genre[ind]=='slate']\n",
    "train_x2_sla = [val for ind, val in enumerate(train_x2) if train_genre[ind]=='slate']\n",
    "train_y_sla = [val for ind, val in enumerate(train_y) if train_genre[ind]=='slate']\n",
    "train_x1_sla_indices = token2index_dataset(train_x1_sla)\n",
    "train_x2_sla_indices = token2index_dataset(train_x2_sla)\n",
    "train_dataset_sla = SNLIDataset(train_x1_sla_indices, train_x2_sla_indices, train_y_sla)\n",
    "train_loader_sla = torch.utils.data.DataLoader(dataset=train_dataset_sla, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters:492603\n",
      "Val Accuracy:43.712574850299404\n"
     ]
    }
   ],
   "source": [
    "model_rnn = torch.load('best_snli_model_rnn.pth')\n",
    "fine_tune_model(model_rnn, train_loader_sla, val_loader_sla, 3e-4, 5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### telephone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x1_tel = [val for ind, val in enumerate(train_x1) if train_genre[ind]=='telephone']\n",
    "train_x2_tel = [val for ind, val in enumerate(train_x2) if train_genre[ind]=='telephone']\n",
    "train_y_tel = [val for ind, val in enumerate(train_y) if train_genre[ind]=='telephone']\n",
    "train_x1_tel_indices = token2index_dataset(train_x1_tel)\n",
    "train_x2_tel_indices = token2index_dataset(train_x2_tel)\n",
    "train_dataset_tel = SNLIDataset(train_x1_tel_indices, train_x2_tel_indices, train_y_tel)\n",
    "train_loader_tel = torch.utils.data.DataLoader(dataset=train_dataset_tel, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters:492603\n",
      "Val Accuracy:47.66169154228856\n"
     ]
    }
   ],
   "source": [
    "model_rnn = torch.load('best_snli_model_rnn.pth')\n",
    "fine_tune_model(model_rnn, train_loader_tel, val_loader_tel, 3e-4, 5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x1_tra = [val for ind, val in enumerate(train_x1) if train_genre[ind]=='travel']\n",
    "train_x2_tra = [val for ind, val in enumerate(train_x2) if train_genre[ind]=='travel']\n",
    "train_y_tra = [val for ind, val in enumerate(train_y) if train_genre[ind]=='travel']\n",
    "train_x1_tra_indices = token2index_dataset(train_x1_tra)\n",
    "train_x2_tra_indices = token2index_dataset(train_x2_tra)\n",
    "train_dataset_tra = SNLIDataset(train_x1_tra_indices, train_x2_tra_indices, train_y_tra)\n",
    "train_loader_tra = torch.utils.data.DataLoader(dataset=train_dataset_tra, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters:492603\n",
      "Val Accuracy:47.04684317718941\n"
     ]
    }
   ],
   "source": [
    "model_rnn = torch.load('best_snli_model_rnn.pth')\n",
    "fine_tune_model(model_rnn, train_loader_tra, val_loader_tra, 3e-4, 5, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
